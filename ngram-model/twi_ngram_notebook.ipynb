{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twi N-gram Language Model\n",
    "\n",
    "This notebook implements an n-gram language model for **Twi** (a low-resource Ghanaian language).\n",
    "\n",
    "**Features:**\n",
    "- Whitespace tokenization (appropriate for Twi)\n",
    "- Validation set tuning for smoothing parameter k\n",
    "- Perplexity evaluation on held-out test set\n",
    "- Autocomplete functionality\n",
    "\n",
    "Adapted from Coursera NLP Specialization Course 2, Week 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"data\"\n",
    "RESULTS_DIR = \"results\"\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"train.twi\")\n",
    "VAL_FILE = os.path.join(DATA_DIR, \"val.twi\")\n",
    "TEST_FILE = os.path.join(DATA_DIR, \"test.twi\")\n",
    "MIN_FREQ = 2  # Minimum word frequency for vocabulary\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing functions defined!\n"
     ]
    }
   ],
   "source": [
    "def load_data(filepath):\n",
    "    \"\"\"Load text data from file (one sentence per line).\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "\n",
    "\n",
    "def split_to_sentences(data):\n",
    "    \"\"\"Split data by newline.\"\"\"\n",
    "    sentences = data.split('\\n')\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Tokenize sentences using whitespace splitting.\n",
    "    More appropriate for Twi than NLTK's English tokenizer.\n",
    "    \"\"\"\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.lower().split()\n",
    "        if tokens:\n",
    "            tokenized_sentences.append(tokens)\n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "def get_tokenized_data(data):\n",
    "    \"\"\"Split data into sentences and tokenize.\"\"\"\n",
    "    sentences = split_to_sentences(data)\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "def count_words(tokenized_sentences):\n",
    "    \"\"\"Count word frequencies.\"\"\"\n",
    "    word_counts = defaultdict(int)\n",
    "    for sentence in tokenized_sentences:\n",
    "        for token in sentence:\n",
    "            word_counts[token] += 1\n",
    "    return dict(word_counts)\n",
    "\n",
    "\n",
    "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
    "    \"\"\"Get words appearing at least count_threshold times.\"\"\"\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "    closed_vocab = [word for word, cnt in word_counts.items() if cnt >= count_threshold]\n",
    "    return closed_vocab\n",
    "\n",
    "\n",
    "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    \"\"\"Replace out-of-vocabulary words with <unk>.\"\"\"\n",
    "    vocabulary = set(vocabulary)\n",
    "    replaced = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        replaced_sentence = [token if token in vocabulary else unknown_token for token in sentence]\n",
    "        replaced.append(replaced_sentence)\n",
    "    return replaced\n",
    "\n",
    "\n",
    "def preprocess_data(train_data, test_data, count_threshold, val_data=None, unknown_token=\"<unk>\"):\n",
    "    \"\"\"\n",
    "    Preprocess train, validation, and test data:\n",
    "    1. Build vocabulary from training data\n",
    "    2. Replace OOV words with <unk>\n",
    "    \"\"\"\n",
    "    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)\n",
    "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary, unknown_token)\n",
    "    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary, unknown_token)\n",
    "    \n",
    "    if val_data is not None:\n",
    "        val_data_replaced = replace_oov_words_by_unk(val_data, vocabulary, unknown_token)\n",
    "        return train_data_replaced, val_data_replaced, test_data_replaced, vocabulary\n",
    "    \n",
    "    return train_data_replaced, test_data_replaced, vocabulary\n",
    "\n",
    "print(\"Preprocessing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. N-gram Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram model functions defined!\n"
     ]
    }
   ],
   "source": [
    "def count_n_grams(data, n, start_token='<s>', end_token='<e>'):\n",
    "    \"\"\"Count all n-grams in the data.\"\"\"\n",
    "    n_grams = defaultdict(int)\n",
    "    \n",
    "    for sentence in data:\n",
    "        sentence = [start_token] * n + sentence + [end_token]\n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        for i in range(len(sentence) - n + 1):\n",
    "            n_gram = sentence[i:i+n]\n",
    "            n_grams[n_gram] += 1\n",
    "    \n",
    "    return dict(n_grams)\n",
    "\n",
    "\n",
    "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, \n",
    "                         vocabulary_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Estimate probability of word given previous n-gram using k-smoothing.\n",
    "    \n",
    "    P(word | previous_n_gram) = (C(previous_n_gram, word) + k) / (C(previous_n_gram) + k*V)\n",
    "    \"\"\"\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
    "    denominator = previous_n_gram_count + (k * vocabulary_size)\n",
    "    \n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n",
    "    numerator = n_plus1_gram_count + k\n",
    "    \n",
    "    probability = numerator / denominator\n",
    "    return probability\n",
    "\n",
    "\n",
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, \n",
    "                          vocabulary, end_token='<e>', unknown_token=\"<unk>\", k=1.0):\n",
    "    \"\"\"Estimate probabilities for all words in vocabulary.\"\"\"\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    vocab_extended = vocabulary + [end_token, unknown_token]\n",
    "    vocabulary_size = len(vocab_extended)\n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocab_extended:\n",
    "        probability = estimate_probability(word, previous_n_gram, n_gram_counts, \n",
    "                                          n_plus1_gram_counts, vocabulary_size, k=k)\n",
    "        probabilities[word] = probability\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "print(\"N-gram model functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perplexity Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity functions defined!\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, \n",
    "                         vocabulary_size, start_token='<s>', end_token='<e>', k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a sentence.\n",
    "    \n",
    "    PP(W) = (∏ 1/P(w_t | w_{t-n}...w_{t-1}))^(1/N)\n",
    "    \"\"\"\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    sentence = [start_token] * n + sentence + [end_token]\n",
    "    sentence = tuple(sentence)\n",
    "    N = len(sentence)\n",
    "    \n",
    "    log_prob_sum = 0.0\n",
    "    \n",
    "    for t in range(n, N):\n",
    "        n_gram = sentence[t-n:t]\n",
    "        word = sentence[t]\n",
    "        \n",
    "        probability = estimate_probability(word, n_gram, n_gram_counts, \n",
    "                                          n_plus1_gram_counts, vocabulary_size, k=k)\n",
    "        \n",
    "        log_prob_sum += math.log(probability)\n",
    "    \n",
    "    perplexity = math.exp(-log_prob_sum / N)\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def calculate_perplexity_on_corpus(test_data, n_gram_counts, n_plus1_gram_counts,\n",
    "                                   vocabulary_size, k=1.0):\n",
    "    \"\"\"Calculate average perplexity over a corpus.\"\"\"\n",
    "    perplexities = []\n",
    "    for sentence in test_data:\n",
    "        if len(sentence) > 0:\n",
    "            pp = calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts,\n",
    "                                     vocabulary_size, k=k)\n",
    "            perplexities.append(pp)\n",
    "    \n",
    "    return sum(perplexities) / len(perplexities) if perplexities else float('inf')\n",
    "\n",
    "print(\"Perplexity functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning function defined!\n"
     ]
    }
   ],
   "source": [
    "def tune_smoothing_parameter(train_data, val_data, vocabulary, n_values=[1, 2, 3], \n",
    "                              k_values=[0.1, 0.5, 1.0, 2.0, 5.0]):\n",
    "    \"\"\"\n",
    "    Tune the smoothing parameter k using validation set.\n",
    "    \n",
    "    Returns:\n",
    "        best_k: optimal smoothing parameter\n",
    "        best_n: optimal n-gram order\n",
    "        tuning_results: detailed results for all combinations\n",
    "    \"\"\"\n",
    "    vocab_size = len(vocabulary) + 2  # +2 for <e> and <unk>\n",
    "    tuning_results = []\n",
    "    \n",
    "    print(f\"Tuning hyperparameters on validation set...\")\n",
    "    print(f\"N-gram orders: {n_values}\")\n",
    "    print(f\"K values: {k_values}\")\n",
    "    \n",
    "    best_pp = float('inf')\n",
    "    best_k = 1.0\n",
    "    best_n = 1\n",
    "    \n",
    "    for n in n_values:\n",
    "        n_gram_counts = count_n_grams(train_data, n)\n",
    "        n_plus1_gram_counts = count_n_grams(train_data, n + 1)\n",
    "        \n",
    "        for k in k_values:\n",
    "            pp = calculate_perplexity_on_corpus(\n",
    "                val_data, n_gram_counts, n_plus1_gram_counts, vocab_size, k=k\n",
    "            )\n",
    "            tuning_results.append({'n': n, 'k': k, 'perplexity': pp})\n",
    "            \n",
    "            if pp < best_pp:\n",
    "                best_pp = pp\n",
    "                best_k = k\n",
    "                best_n = n\n",
    "    \n",
    "    print(f\"\\nBest: {best_n}-gram with k={best_k} (val perplexity={best_pp:.2f})\")\n",
    "    \n",
    "    return best_k, best_n, tuning_results\n",
    "\n",
    "print(\"Tuning function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Autocomplete Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocomplete functions defined!\n"
     ]
    }
   ],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, \n",
    "                   vocabulary, k=1.0, start_with=None):\n",
    "    \"\"\"Suggest the most likely next word.\"\"\"\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    previous_tokens = ['<s>'] * n + previous_tokens\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    \n",
    "    probabilities = estimate_probabilities(previous_n_gram, n_gram_counts, \n",
    "                                          n_plus1_gram_counts, vocabulary, k=k)\n",
    "    \n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "    \n",
    "    for word, prob in probabilities.items():\n",
    "        if start_with and not word.startswith(start_with):\n",
    "            continue\n",
    "        if prob > max_prob:\n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "    \n",
    "    return suggestion, max_prob\n",
    "\n",
    "\n",
    "def get_top_k_suggestions(previous_tokens, n_gram_counts, n_plus1_gram_counts,\n",
    "                          vocabulary, k=1.0, top_k=5):\n",
    "    \"\"\"Get top-k word suggestions.\"\"\"\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    previous_tokens = ['<s>'] * n + previous_tokens\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    \n",
    "    probabilities = estimate_probabilities(previous_n_gram, n_gram_counts,\n",
    "                                          n_plus1_gram_counts, vocabulary, k=k)\n",
    "    \n",
    "    sorted_probs = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_probs[:top_k]\n",
    "\n",
    "print(\"Autocomplete functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Tokenizing...\n",
      "Training sentences: 29,003\n",
      "Validation sentences: 800\n",
      "Test sentences: 1,300\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_raw = load_data(TRAIN_FILE)\n",
    "val_raw = load_data(VAL_FILE)\n",
    "test_raw = load_data(TEST_FILE)\n",
    "\n",
    "# Tokenize\n",
    "print(\"Tokenizing...\")\n",
    "train_tokenized = get_tokenized_data(train_raw)\n",
    "val_tokenized = get_tokenized_data(val_raw)\n",
    "test_tokenized = get_tokenized_data(test_raw)\n",
    "\n",
    "print(f\"Training sentences: {len(train_tokenized):,}\")\n",
    "print(f\"Validation sentences: {len(val_tokenized):,}\")\n",
    "print(f\"Test sentences: {len(test_tokenized):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing (min_freq=2)...\n",
      "Vocabulary size: 13,194\n",
      "OOV rate on test set: 2.45%\n"
     ]
    }
   ],
   "source": [
    "# Preprocess (handle OOV)\n",
    "print(f\"Preprocessing (min_freq={MIN_FREQ})...\")\n",
    "train_data, val_data, test_data, vocabulary = preprocess_data(\n",
    "    train_tokenized, test_tokenized, MIN_FREQ, val_data=val_tokenized\n",
    ")\n",
    "print(f\"Vocabulary size: {len(vocabulary):,}\")\n",
    "\n",
    "# Calculate OOV rate\n",
    "test_tokens = [t for s in test_tokenized for t in s]\n",
    "oov_count = sum(1 for t in test_tokens if t not in set(vocabulary))\n",
    "oov_rate = oov_count / len(test_tokens) * 100\n",
    "print(f\"OOV rate on test set: {oov_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tune Hyperparameters on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyperparameters on validation set...\n",
      "N-gram orders: [1, 2, 3, 4, 5]\n",
      "K values: [0.01, 0.1, 0.5, 1.0, 2.0]\n",
      "\n",
      "Best: 1-gram with k=0.01 (val perplexity=303.77)\n"
     ]
    }
   ],
   "source": [
    "# Tune k on validation set\n",
    "best_k, best_n, tuning_results = tune_smoothing_parameter(\n",
    "    train_data, val_data, vocabulary,\n",
    "    n_values=[1, 2, 3, 4, 5],\n",
    "    k_values=[0.01, 0.1, 0.5, 1.0, 2.0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train and Evaluate N-gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating on test set with tuned k={best_k}...\\n\")\n",
    "\n",
    "results = []\n",
    "n_gram_counts_list = []\n",
    "vocab_size = len(vocabulary) + 2\n",
    "\n",
    "for n in range(1, 6):\n",
    "    print(f\"Evaluating {n}-gram model...\")\n",
    "    n_gram_counts = count_n_grams(train_data, n)\n",
    "    n_plus1_gram_counts = count_n_grams(train_data, n + 1)\n",
    "    n_gram_counts_list.append((n_gram_counts, n_plus1_gram_counts))\n",
    "    \n",
    "    print(f\"  Unique {n}-grams: {len(n_gram_counts):,}\")\n",
    "    \n",
    "    perplexity = calculate_perplexity_on_corpus(\n",
    "        test_data, n_gram_counts, n_plus1_gram_counts, vocab_size, k=best_k\n",
    "    )\n",
    "    \n",
    "    print(f\"  Perplexity: {perplexity:.2f}\\n\")\n",
    "    results.append({'n': n, 'perplexity': perplexity, 'n_grams': len(n_gram_counts)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot perplexity vs n-gram order\n",
    "n_values = [r['n'] for r in results]\n",
    "perplexities = [r['perplexity'] for r in results]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_values, perplexities, 'bo-', linewidth=2, markersize=10)\n",
    "plt.xlabel('N-gram Order', fontsize=12)\n",
    "plt.ylabel('Perplexity', fontsize=12)\n",
    "plt.title(f'Twi N-gram Language Model: Perplexity vs N-gram Order (k={best_k})', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "for n, pp in zip(n_values, perplexities):\n",
    "    plt.annotate(f'{pp:.1f}', (n, pp), textcoords=\"offset points\", \n",
    "                xytext=(0, 10), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"perplexity_plot.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERPLEXITY SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Tuned smoothing parameter: k = {best_k}\")\n",
    "print(f\"Validation sentences: {len(val_data):,}\")\n",
    "print(\"\\n{:<12} {:>12} {:>15}\".format('N-gram', 'Perplexity', 'Unique N-grams'))\n",
    "print(\"-\"*40)\n",
    "\n",
    "best_test_n = min(results, key=lambda x: x['perplexity'])['n']\n",
    "for r in results:\n",
    "    marker = \" <-- Best\" if r['n'] == best_test_n else \"\"\n",
    "    print(f\"{r['n']}-gram{marker:<10} {r['perplexity']:>10.2f} {r['n_grams']:>15,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Autocomplete Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model for autocomplete\n",
    "best_n_gram_counts, best_n_plus1_gram_counts = n_gram_counts_list[best_test_n - 1]\n",
    "\n",
    "# Test autocomplete with some Twi phrases\n",
    "sample_inputs = [\n",
    "    [\"na\"],                  # \"and\"\n",
    "    [\"awurade\"],             # \"Lord\"\n",
    "    [\"na\", \"ɔka\"],           # \"and he said\"\n",
    "    [\"me\", \"nyankopɔn\"],     # \"my God\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AUTOCOMPLETE DEMO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for input_tokens in sample_inputs:\n",
    "    suggestions = get_top_k_suggestions(\n",
    "        input_tokens, best_n_gram_counts, best_n_plus1_gram_counts,\n",
    "        vocabulary, k=best_k, top_k=5\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nInput: '{' '.join(input_tokens)}'\")\n",
    "    print(\"Top suggestions:\")\n",
    "    for word, prob in suggestions:\n",
    "        print(f\"  - {word}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Interactive Autocomplete\n",
    "\n",
    "Try your own inputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive autocomplete - modify this cell to try different inputs!\n",
    "your_input = [\"na\", \"awurade\"]  # <-- Change this to try different inputs\n",
    "\n",
    "suggestions = get_top_k_suggestions(\n",
    "    your_input, best_n_gram_counts, best_n_plus1_gram_counts,\n",
    "    vocabulary, k=best_k, top_k=10\n",
    ")\n",
    "\n",
    "print(f\"Input: '{' '.join(your_input)}'\")\n",
    "print(\"\\nTop 10 suggestions:\")\n",
    "for i, (word, prob) in enumerate(suggestions, 1):\n",
    "    print(f\"  {i}. {word}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Vocabulary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 20 most frequent words\n",
    "word_counts = count_words(train_data)\n",
    "sorted_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VOCABULARY STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nTotal vocabulary size: {len(vocabulary):,}\")\n",
    "print(f\"Training tokens: {sum(len(s) for s in train_data):,}\")\n",
    "print(f\"Test tokens: {sum(len(s) for s in test_data):,}\")\n",
    "\n",
    "print(\"\\nTop 20 most frequent words:\")\n",
    "print(\"-\"*30)\n",
    "for i, (word, count) in enumerate(sorted_counts[:20], 1):\n",
    "    print(f\"{i:2}. {word}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated building an n-gram language model for Twi, a low-resource Ghanaian language.\n",
    "\n",
    "**Key findings:**\n",
    "- Validation set tuning found optimal k=0.01\n",
    "- Unigram achieved lowest perplexity due to data sparsity\n",
    "- Higher-order n-grams suffer from insufficient training data\n",
    "- Autocomplete successfully predicts meaningful Twi word patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
