{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": [], "gpuType": "T4"},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": ["# AgricGPT - Fine-tuning with CROP-benchmark Evaluation\n\n", "- **QLoRA** fine-tuning on AI4Agr/CROP-dataset\n", "- **Evaluation** using official CROP-benchmark (accuracy metrics)\n", "- **Model card** with benchmark results pushed to HuggingFace"],
      "metadata": {"id": "intro"}
    },
    {
      "cell_type": "code",
      "source": ["%pip install -q torch transformers datasets peft bitsandbytes accelerate huggingface_hub"],
      "metadata": {"id": "install"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["from huggingface_hub import login\n", "login()"],
      "metadata": {"id": "login"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Configuration"],
      "metadata": {"id": "config_header"}
    },
    {
      "cell_type": "code",
      "source": ["import torch, math, json, re, os\nfrom collections import defaultdict\n\n# Model\nMODEL_NAME = \"microsoft/phi-2\"\nOUTPUT_DIR = \"./agri_model_results\"\n\n# HuggingFace\nHF_MODEL_NAME = \"agricgpt-phi2\"\nPUSH_TO_HUB = True\nSAVE_STEPS = 100\n\n# Data\nDATASET_SIZE = 5000\nVALIDATION_SPLIT = 0.1\nMAX_SEQ_LENGTH = 512\nBENCHMARK_SAMPLE_SIZE = 500  # English benchmark questions\n\n# LoRA\nLORA_R, LORA_ALPHA, LORA_DROPOUT = 16, 32, 0.05\nTARGET_MODULES = [\"fc1\", \"fc2\", \"q_proj\", \"k_proj\", \"v_proj\", \"dense\"]\n\n# Training\nNUM_EPOCHS, BATCH_SIZE = 3, 2\nGRADIENT_ACCUMULATION_STEPS = 4\nLEARNING_RATE = 2e-4\nLOGGING_STEPS, EVAL_STEPS = 10, 50\n\nassert torch.cuda.is_available(), \"GPU required!\"\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")"],
      "metadata": {"id": "config"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Load Model"],
      "metadata": {"id": "model_header"}
    },
    {
      "cell_type": "code",
      "source": ["from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig, pipeline\n\nbnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, trust_remote_code=True, device_map={\"\":0})\nmodel.config.use_cache = False\nprint(f\"Loaded: {MODEL_NAME}\")"],
      "metadata": {"id": "load_model"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Load CROP-benchmark (English)"],
      "metadata": {"id": "benchmark_header"}
    },
    {
      "cell_type": "code",
      "source": ["from datasets import load_dataset\n\nbenchmark = load_dataset(\"AI4Agr/CROP-benchmark\", split=\"train\")\n\ndef is_english(text):\n    if not text: return False\n    return sum(1 for c in text if ord(c) < 128) / len(text) > 0.7\n\nenglish_benchmark = benchmark.filter(lambda x: is_english(x.get(\"Question\", \"\")))\nprint(f\"English questions: {len(english_benchmark)}\")\n\nif BENCHMARK_SAMPLE_SIZE:\n    english_benchmark = english_benchmark.shuffle(seed=42).select(range(min(BENCHMARK_SAMPLE_SIZE, len(english_benchmark))))\n    print(f\"Using {len(english_benchmark)} for evaluation\")"],
      "metadata": {"id": "load_benchmark"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## MCQ Evaluation Functions"],
      "metadata": {"id": "eval_funcs_header"}
    },
    {
      "cell_type": "code",
      "source": ["def format_mcq_prompt(q, opts):\n    return f\"\"\"### Instruction:\nAnswer the following agricultural question by selecting the correct option (A, B, C, or D).\n\nQuestion: {q}\n\nA) {opts['A']}\nB) {opts['B']}\nC) {opts['C']}\nD) {opts['D']}\n\nReply with only the letter of the correct answer.\n\n### Response:\n\"\"\"\n\ndef extract_answer(response):\n    response = response.strip().upper()\n    match = re.search(r'\\b([ABCD])\\b', response)\n    return match.group(1) if match else (response[0] if response and response[0] in 'ABCD' else None)\n\ndef evaluate_mcq(pipe, gen_config, data):\n    correct, total = 0, 0\n    by_level = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n    \n    for item in data:\n        q = item.get(\"Question\", \"\")\n        opts = {\"A\": item.get(\"Option A\", \"\"), \"B\": item.get(\"Option B\", \"\"), \"C\": item.get(\"Option C\", \"\"), \"D\": item.get(\"Option D\", \"\")}\n        ans = item.get(\"Answer\", \"\").strip().upper()\n        level = item.get(\"Level\", \"Unknown\")\n        \n        if not q or not ans: continue\n        \n        torch.manual_seed(42)\n        result = pipe(format_mcq_prompt(q, opts), generation_config=gen_config)\n        pred = extract_answer(result[0]['generated_text'].split(\"### Response:\")[-1])\n        \n        if pred == ans:\n            correct += 1\n            by_level[level][\"correct\"] += 1\n        total += 1\n        by_level[level][\"total\"] += 1\n    \n    return {\"accuracy\": correct/total if total else 0, \"correct\": correct, \"total\": total, \"by_level\": dict(by_level)}\n\nprint(\"Evaluation functions ready\")"],
      "metadata": {"id": "eval_funcs"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Base Model Evaluation (BEFORE Training)"],
      "metadata": {"id": "base_eval_header"}
    },
    {
      "cell_type": "code",
      "source": ["base_pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\ngen_config = GenerationConfig(max_new_tokens=10, do_sample=False, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id)\n\nprint(\"Evaluating base model on CROP-benchmark...\")\nbase_results = evaluate_mcq(base_pipe, gen_config, english_benchmark)\nprint(f\"\\nðŸ“Š Base Model Accuracy: {base_results['accuracy']:.2%} ({base_results['correct']}/{base_results['total']})\")"],
      "metadata": {"id": "base_eval"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Prepare Training Data"],
      "metadata": {"id": "data_header"}
    },
    {
      "cell_type": "code",
      "source": ["dataset = load_dataset(\"AI4Agr/CROP-dataset\", data_files=\"**/*_en/**/*.json\", split=\"train\")\nif DATASET_SIZE: dataset = dataset.select(range(min(DATASET_SIZE, len(dataset))))\n\ndef format_instruction(s):\n    return {\"text\": f\"### Instruction:\\n{s['instruction']}\\n\\n### Response:\\n{s['output']}{tokenizer.eos_token}\"}\n\ndataset = dataset.map(format_instruction)\ndataset = dataset.train_test_split(test_size=VALIDATION_SPLIT, seed=42)\ntrain_ds, eval_ds = dataset[\"train\"], dataset[\"test\"]\n\ndef tokenize(examples):\n    return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_SEQ_LENGTH, padding=\"max_length\")\n\ntok_train = train_ds.map(tokenize, batched=True, remove_columns=train_ds.column_names)\ntok_eval = eval_ds.map(tokenize, batched=True, remove_columns=eval_ds.column_names)\nprint(f\"Train: {len(tok_train)}, Eval: {len(tok_eval)}\")"],
      "metadata": {"id": "prepare_data"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Configure LoRA"],
      "metadata": {"id": "lora_header"}
    },
    {
      "cell_type": "code",
      "source": ["from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nmodel = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(r=LORA_R, lora_alpha=LORA_ALPHA, target_modules=TARGET_MODULES, lora_dropout=LORA_DROPOUT, bias=\"none\", task_type=\"CAUSAL_LM\")\nmodel = get_peft_model(model, peft_config)\n\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")"],
      "metadata": {"id": "lora"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Training"],
      "metadata": {"id": "train_header"}
    },
    {
      "cell_type": "code",
      "source": ["from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR, num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE, per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, learning_rate=LEARNING_RATE,\n    logging_steps=LOGGING_STEPS, fp16=True, optim=\"paged_adamw_32bit\",\n    warmup_ratio=0.03, lr_scheduler_type=\"cosine\",\n    eval_strategy=\"steps\", eval_steps=EVAL_STEPS,\n    save_strategy=\"steps\", save_steps=SAVE_STEPS, save_total_limit=3,\n    load_best_model_at_end=True, metric_for_best_model=\"eval_loss\", greater_is_better=False,\n    push_to_hub=PUSH_TO_HUB, hub_model_id=HF_MODEL_NAME if PUSH_TO_HUB else None,\n    hub_strategy=\"every_save\", report_to=\"none\", seed=42\n)\n\ntrainer = Trainer(model=model, train_dataset=tok_train, eval_dataset=tok_eval, args=training_args,\n                  data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False))\n\nprint(\"Starting training...\")\ntrainer.train()"],
      "metadata": {"id": "train"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Fine-tuned Model Evaluation (AFTER Training)"],
      "metadata": {"id": "ft_eval_header"}
    },
    {
      "cell_type": "code",
      "source": ["from transformers import logging\nlogging.set_verbosity(logging.CRITICAL)\nmodel.eval()\n\nft_pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n\nprint(\"Evaluating fine-tuned model on CROP-benchmark...\")\nft_results = evaluate_mcq(ft_pipe, gen_config, english_benchmark)\nprint(f\"\\nðŸ“Š Fine-tuned Accuracy: {ft_results['accuracy']:.2%} ({ft_results['correct']}/{ft_results['total']})\")"],
      "metadata": {"id": "ft_eval"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Evaluation Summary"],
      "metadata": {"id": "summary_header"}
    },
    {
      "cell_type": "code",
      "source": ["# Perplexity\ndef calc_ppl(model, tokenizer, texts):\n    model.eval()\n    total_loss, total_tokens = 0, 0\n    with torch.no_grad():\n        for text in texts[:100]:\n            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n            out = model(**inputs, labels=inputs[\"input_ids\"])\n            total_loss += out.loss.item() * inputs[\"input_ids\"].size(1)\n            total_tokens += inputs[\"input_ids\"].size(1)\n    return math.exp(total_loss / total_tokens)\n\nperplexity = calc_ppl(model, tokenizer, [s[\"text\"] for s in eval_ds])\n\n# Get losses\nhistory = trainer.state.log_history\ntrain_losses = [(h['step'], h['loss']) for h in history if 'loss' in h and 'eval_loss' not in h]\neval_losses = [(h['step'], h['eval_loss']) for h in history if 'eval_loss' in h]\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EVALUATION SUMMARY\")\nprint(\"=\"*60)\nprint(f\"\\nðŸ“‰ Training Loss: {train_losses[0][1]:.4f} â†’ {train_losses[-1][1]:.4f}\")\nprint(f\"ðŸ“ˆ Validation Loss: {eval_losses[0][1]:.4f} â†’ {min(e[1] for e in eval_losses):.4f}\")\nprint(f\"ðŸŽ¯ Perplexity: {perplexity:.2f}\")\nprint(f\"\\nðŸ“‹ CROP-benchmark Results:\")\nprint(f\"   Base Model:      {base_results['accuracy']:.2%}\")\nprint(f\"   Fine-tuned:      {ft_results['accuracy']:.2%}\")\nprint(f\"   Improvement:     +{(ft_results['accuracy']-base_results['accuracy'])*100:.1f}%\")\n\nprint(f\"\\nðŸ“Š By Difficulty:\")\nfor level, stats in sorted(ft_results['by_level'].items()):\n    acc = stats['correct']/stats['total'] if stats['total'] else 0\n    print(f\"   Level {level}: {acc:.2%} ({stats['correct']}/{stats['total']})\")"],
      "metadata": {"id": "summary"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Save & Push to HuggingFace"],
      "metadata": {"id": "push_header"}
    },
    {
      "cell_type": "code",
      "source": ["# Save results\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nresults = {\n    \"benchmark\": \"CROP-benchmark\",\n    \"base_accuracy\": base_results['accuracy'],\n    \"finetuned_accuracy\": ft_results['accuracy'],\n    \"perplexity\": perplexity,\n    \"best_eval_loss\": min(e[1] for e in eval_losses),\n    \"accuracy_by_level\": {k: v['correct']/v['total'] for k,v in ft_results['by_level'].items() if v['total']>0}\n}\nwith open(f\"{OUTPUT_DIR}/evaluation_results.json\", \"w\") as f:\n    json.dump(results, f, indent=2)\n\nif PUSH_TO_HUB:\n    from huggingface_hub import HfApi\n    \n    # Model card\n    card = f\"\"\"---\nlanguage: [en]\nlicense: apache-2.0\ntags: [agriculture, phi-2, qlora, crop-science]\ndatasets: [AI4Agr/CROP-dataset]\nbase_model: microsoft/phi-2\nmodel-index:\n- name: AgricGPT-Phi2\n  results:\n  - task: {{type: question-answering, name: Agricultural MCQ}}\n    dataset: {{name: CROP-benchmark, type: AI4Agr/CROP-benchmark}}\n    metrics:\n    - {{type: accuracy, value: {ft_results['accuracy']*100:.1f}, name: Accuracy}}\n---\n\n# AgricGPT - Agricultural QA Model\n\n## Benchmark Results (CROP-benchmark)\n\n| Model | Accuracy |\n|-------|----------|\n| Base Phi-2 | {base_results['accuracy']*100:.1f}% |\n| **AgricGPT** | **{ft_results['accuracy']*100:.1f}%** (+{(ft_results['accuracy']-base_results['accuracy'])*100:.1f}%) |\n\n| Metric | Value |\n|--------|-------|\n| Perplexity | {perplexity:.2f} |\n| Val Loss | {results['best_eval_loss']:.4f} |\n\"\"\"\n    with open(f\"{OUTPUT_DIR}/README.md\", \"w\") as f:\n        f.write(card)\n    \n    trainer.push_to_hub()\n    api = HfApi()\n    api.upload_file(f\"{OUTPUT_DIR}/README.md\", \"README.md\", f\"{api.whoami()['name']}/{HF_MODEL_NAME}\", \"model\")\n    print(f\"\\nâœ… Done! https://huggingface.co/{api.whoami()['name']}/{HF_MODEL_NAME}\")"],
      "metadata": {"id": "push"},
      "execution_count": null,
      "outputs": []
    }
  ]
}
