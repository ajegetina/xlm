{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AgricGPT - Agricultural Domain Instruction Tuning with QLoRA\n",
        "\n",
        "This notebook fine-tunes **Microsoft Phi-2** on the **AI4Agr/CROP-dataset** for agricultural Q&A using:\n",
        "- **QLoRA** (4-bit quantization + Low-Rank Adaptation)\n",
        "- **Instruction tuning** format\n",
        "\n",
        "**Requirements**: T4 GPU or better"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "Run this cell and **restart the runtime** if prompted."
      ],
      "metadata": {
        "id": "install_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "    torch>=2.0.0 \\\n",
        "    transformers>=4.40.0 \\\n",
        "    datasets>=2.0.0 \\\n",
        "    peft>=0.10.0 \\\n",
        "    bitsandbytes>=0.43.0 \\\n",
        "    accelerate>=0.27.0 \\\n",
        "    huggingface_hub"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Configuration"
      ],
      "metadata": {
        "id": "config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Model\n",
        "MODEL_NAME = \"microsoft/phi-2\"\n",
        "OUTPUT_DIR = \"./agri_model_results\"\n",
        "\n",
        "# Hugging Face Hub\n",
        "HF_MODEL_NAME = \"agricgpt-phi2\"  # <- Change this!\n",
        "\n",
        "# Dataset\n",
        "DATASET_SIZE = 5000  # Set to None for full dataset\n",
        "MAX_SEQ_LENGTH = 512\n",
        "\n",
        "# LoRA\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "TARGET_MODULES = [\"fc1\", \"fc2\", \"q_proj\", \"k_proj\", \"v_proj\", \"dense\"]\n",
        "\n",
        "# Training\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 2\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "LOGGING_STEPS = 10\n",
        "\n",
        "# Check GPU\n",
        "if not torch.cuda.is_available():\n",
        "    raise ValueError(\"GPU required!\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load Model with 4-bit Quantization"
      ],
      "metadata": {
        "id": "model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        "    device_map={\"\":0}\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "print(f\"Model loaded: {MODEL_NAME}\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Base Model Output (BEFORE Training)\n",
        "\n",
        "Let's see how the model responds **before** fine-tuning on agricultural data."
      ],
      "metadata": {
        "id": "base_model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GenerationConfig, pipeline\n",
        "\n",
        "# Create pipeline for base model\n",
        "base_pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "base_gen_config = GenerationConfig(\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Test base model with agricultural question\n",
        "torch.manual_seed(42)\n",
        "test_prompt = \"### Instruction:\\nWhat is crop rotation?\\n\\n### Response:\\n\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BASE MODEL OUTPUT (before training)\")\n",
        "print(\"=\" * 60)\n",
        "result = base_pipe(test_prompt, generation_config=base_gen_config)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "base_model_test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load and Prepare Dataset"
      ],
      "metadata": {
        "id": "dataset_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"AI4Agr/CROP-dataset\",\n",
        "    data_files=\"**/*_en/**/*.json\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "if DATASET_SIZE:\n",
        "    dataset = dataset.select(range(min(DATASET_SIZE, len(dataset))))\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)} samples\")"
      ],
      "metadata": {
        "id": "load_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_instruction(sample):\n",
        "    prompt = (\n",
        "        f\"### Instruction:\\n{sample['instruction']}\\n\\n\"\n",
        "        f\"### Response:\\n{sample['output']}{tokenizer.eos_token}\"\n",
        "    )\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "dataset = dataset.map(format_instruction)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_SEQ_LENGTH, padding=\"max_length\")\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
        "print(f\"Tokenized: {len(tokenized_dataset)} samples\")"
      ],
      "metadata": {
        "id": "format_tokenize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Configure LoRA"
      ],
      "metadata": {
        "id": "lora_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")"
      ],
      "metadata": {
        "id": "lora_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training"
      ],
      "metadata": {
        "id": "training_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Fine-Tuned Model Output (AFTER Training)"
      ],
      "metadata": {
        "id": "post_training_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import logging\n",
        "\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "model.eval()\n",
        "\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=256,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "def ask_agrigpt(question: str) -> str:\n",
        "    prompt = f\"### Instruction:\\n{question}\\n\\n### Response:\\n\"\n",
        "    result = pipe(prompt, generation_config=generation_config)\n",
        "    response = result[0]['generated_text'].split(\"### Response:\\n\")[-1]\n",
        "    return response.split(\"### Instruction:\")[0].strip()\n",
        "\n",
        "# Test with SAME question as before training\n",
        "torch.manual_seed(42)\n",
        "print(\"=\" * 60)\n",
        "print(\"FINE-TUNED MODEL OUTPUT (after training)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Q: What is crop rotation?\")\n",
        "print(f\"A: {ask_agrigpt('What is crop rotation?')}\")"
      ],
      "metadata": {
        "id": "post_training_test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. More Test Questions"
      ],
      "metadata": {
        "id": "more_tests_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"How can I prevent soil erosion on my farm?\",\n",
        "    \"What are the benefits of organic farming?\",\n",
        "    \"How do I manage pests naturally?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {ask_agrigpt(q)}\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "more_tests"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Save & Push to Hugging Face Hub ðŸ¤—"
      ],
      "metadata": {
        "id": "save_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save locally\n",
        "save_path = f\"{OUTPUT_DIR}/final_model\"\n",
        "trainer.save_model(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "print(f\"Saved to: {save_path}\")"
      ],
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()  # Enter your HF token"
      ],
      "metadata": {
        "id": "hf_login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Pushing to Hugging Face as '{HF_MODEL_NAME}'...\")\n",
        "model.push_to_hub(HF_MODEL_NAME)\n",
        "tokenizer.push_to_hub(HF_MODEL_NAME)\n",
        "print(f\"âœ… Done! View at: https://huggingface.co/YOUR_USERNAME/{HF_MODEL_NAME}\")"
      ],
      "metadata": {
        "id": "push_to_hub"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
