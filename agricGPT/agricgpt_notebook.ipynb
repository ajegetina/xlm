{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AgricGPT - Agricultural Domain Instruction Tuning with QLoRA\n",
        "\n",
        "Fine-tunes **Microsoft Phi-2** on **AI4Agr/CROP-dataset** with:\n",
        "- QLoRA (4-bit quantization)\n",
        "- Train/Validation split with loss tracking\n",
        "- Perplexity evaluation\n",
        "- Model card and metrics pushed to HuggingFace"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Dependencies"
      ],
      "metadata": {
        "id": "install_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q torch transformers datasets peft bitsandbytes accelerate huggingface_hub"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Login to Hugging Face"
      ],
      "metadata": {
        "id": "hf_login_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "hf_login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Configuration"
      ],
      "metadata": {
        "id": "config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import json\n",
        "\n",
        "# Model\n",
        "MODEL_NAME = \"microsoft/phi-2\"\n",
        "OUTPUT_DIR = \"./agri_model_results\"\n",
        "\n",
        "# Hugging Face Hub\n",
        "HF_MODEL_NAME = \"agricgpt-phi2\"\n",
        "PUSH_TO_HUB = True\n",
        "SAVE_STEPS = 100\n",
        "\n",
        "# Dataset\n",
        "DATASET_SIZE = 5000\n",
        "VALIDATION_SPLIT = 0.1\n",
        "MAX_SEQ_LENGTH = 512\n",
        "\n",
        "# LoRA\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "TARGET_MODULES = [\"fc1\", \"fc2\", \"q_proj\", \"k_proj\", \"v_proj\", \"dense\"]\n",
        "\n",
        "# Training\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 2\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "LOGGING_STEPS = 10\n",
        "EVAL_STEPS = 50\n",
        "\n",
        "assert torch.cuda.is_available(), \"GPU required!\"\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Load Model"
      ],
      "metadata": {
        "id": "model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        "    device_map={\"\":0}\n",
        ")\n",
        "model.config.use_cache = False\n",
        "print(f\"Loaded: {MODEL_NAME}\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Test Questions (20 for Before/After Comparison)"
      ],
      "metadata": {
        "id": "test_questions_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_QUESTIONS = [\n",
        "    \"What is crop rotation and why is it important?\",\n",
        "    \"How do I know when my maize is ready for harvest?\",\n",
        "    \"What are cover crops and how do they help?\",\n",
        "    \"How should I prepare soil before planting?\",\n",
        "    \"What is the best time to plant tomatoes?\",\n",
        "    \"How can I control aphids naturally without chemicals?\",\n",
        "    \"What causes leaf blight in potatoes?\",\n",
        "    \"How do I prevent fungal diseases in my crops?\",\n",
        "    \"What are the signs of pest infestation in stored grains?\",\n",
        "    \"How can I manage weeds organically?\",\n",
        "    \"How can I improve soil fertility naturally?\",\n",
        "    \"What is the difference between organic and inorganic fertilizers?\",\n",
        "    \"How do I test my soil pH?\",\n",
        "    \"What nutrients do plants need most?\",\n",
        "    \"How can I prevent soil erosion on my farm?\",\n",
        "    \"What is drip irrigation and what are its benefits?\",\n",
        "    \"How much water do vegetable crops need?\",\n",
        "    \"How can I conserve water on my farm?\",\n",
        "    \"What are the benefits of organic farming?\",\n",
        "    \"How do I start a small vegetable garden?\"\n",
        "]\n",
        "print(f\"Defined {len(TEST_QUESTIONS)} test questions\")"
      ],
      "metadata": {
        "id": "test_questions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Base Model Evaluation (BEFORE Training)"
      ],
      "metadata": {
        "id": "base_eval_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GenerationConfig, pipeline\n",
        "\n",
        "base_pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "gen_config = GenerationConfig(\n",
        "    max_new_tokens=150, do_sample=True, temperature=0.7,\n",
        "    top_p=0.9, repetition_penalty=1.2,\n",
        "    eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "def generate_response(pipe, question):\n",
        "    prompt = f\"### Instruction:\\n{question}\\n\\n### Response:\\n\"\n",
        "    result = pipe(prompt, generation_config=gen_config)\n",
        "    response = result[0]['generated_text'].split(\"### Response:\\n\")[-1]\n",
        "    return response.split(\"### Instruction:\")[0].strip()[:500]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BASE MODEL RESPONSES (before training)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "all_base_responses = []\n",
        "for i, q in enumerate(TEST_QUESTIONS, 1):\n",
        "    torch.manual_seed(42)\n",
        "    response = generate_response(base_pipe, q)\n",
        "    all_base_responses.append(response)\n",
        "    if i <= 3:\n",
        "        print(f\"\\nQ{i}: {q}\")\n",
        "        print(f\"A: {response[:200]}...\")\n",
        "\n",
        "print(f\"\\nCollected {len(all_base_responses)} base responses\")"
      ],
      "metadata": {
        "id": "base_eval"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Prepare Dataset with Train/Validation Split"
      ],
      "metadata": {
        "id": "dataset_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"AI4Agr/CROP-dataset\", data_files=\"**/*_en/**/*.json\", split=\"train\")\n",
        "if DATASET_SIZE:\n",
        "    dataset = dataset.select(range(min(DATASET_SIZE, len(dataset))))\n",
        "\n",
        "def format_instruction(sample):\n",
        "    return {\"text\": f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']}{tokenizer.eos_token}\"}\n",
        "\n",
        "dataset = dataset.map(format_instruction)\n",
        "dataset = dataset.train_test_split(test_size=VALIDATION_SPLIT, seed=42)\n",
        "train_dataset, eval_dataset = dataset[\"train\"], dataset[\"test\"]\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_SEQ_LENGTH, padding=\"max_length\")\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_eval = eval_dataset.map(tokenize_function, batched=True, remove_columns=eval_dataset.column_names)\n",
        "\n",
        "print(f\"Train: {len(tokenized_train)}, Eval: {len(tokenized_eval)}\")"
      ],
      "metadata": {
        "id": "prepare_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Configure LoRA"
      ],
      "metadata": {
        "id": "lora_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "peft_config = LoraConfig(\n",
        "    r=LORA_R, lora_alpha=LORA_ALPHA, target_modules=TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")"
      ],
      "metadata": {
        "id": "lora_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Training with Validation Loss Tracking"
      ],
      "metadata": {
        "id": "training_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=PUSH_TO_HUB,\n",
        "    hub_model_id=HF_MODEL_NAME if PUSH_TO_HUB else None,\n",
        "    hub_strategy=\"every_save\",\n",
        "    report_to=\"none\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Perplexity Calculation"
      ],
      "metadata": {
        "id": "perplexity_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(model, tokenizer, texts, max_length=512):\n",
        "    model.eval()\n",
        "    total_loss, total_tokens = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for text in texts:\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "            total_loss += outputs.loss.item() * inputs[\"input_ids\"].size(1)\n",
        "            total_tokens += inputs[\"input_ids\"].size(1)\n",
        "    return math.exp(total_loss / total_tokens)\n",
        "\n",
        "eval_texts = [s[\"text\"] for s in eval_dataset]\n",
        "perplexity = calculate_perplexity(model, tokenizer, eval_texts[:100])\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"PERPLEXITY: {perplexity:.2f}\")\n",
        "print(f\"{'='*50}\")"
      ],
      "metadata": {
        "id": "perplexity"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Fine-Tuned Model Evaluation (AFTER Training)"
      ],
      "metadata": {
        "id": "finetuned_eval_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import logging\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "model.eval()\n",
        "\n",
        "ft_pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FINE-TUNED MODEL RESPONSES (after training)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "all_ft_responses = []\n",
        "for i, q in enumerate(TEST_QUESTIONS, 1):\n",
        "    torch.manual_seed(42)\n",
        "    response = generate_response(ft_pipe, q)\n",
        "    all_ft_responses.append(response)\n",
        "    if i <= 3:\n",
        "        print(f\"\\nQ{i}: {q}\")\n",
        "        print(f\"A: {response[:200]}...\")\n",
        "\n",
        "print(f\"\\nCollected {len(all_ft_responses)} fine-tuned responses\")"
      ],
      "metadata": {
        "id": "finetuned_eval"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Side-by-Side Comparison"
      ],
      "metadata": {
        "id": "comparison_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"BEFORE vs AFTER COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, (q, base_r, ft_r) in enumerate(zip(TEST_QUESTIONS, all_base_responses, all_ft_responses), 1):\n",
        "    print(f\"\\n{'â”€'*80}\")\n",
        "    print(f\"Q{i}: {q}\")\n",
        "    print(f\"\\nðŸ“Œ BEFORE: {base_r[:150]}...\" if len(base_r) > 150 else f\"\\nðŸ“Œ BEFORE: {base_r}\")\n",
        "    print(f\"\\nâœ… AFTER: {ft_r[:150]}...\" if len(ft_r) > 150 else f\"\\nâœ… AFTER: {ft_r}\")"
      ],
      "metadata": {
        "id": "comparison"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Evaluation Summary"
      ],
      "metadata": {
        "id": "summary_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = trainer.state.log_history\n",
        "train_losses = [(h['step'], h['loss']) for h in history if 'loss' in h and 'eval_loss' not in h]\n",
        "eval_losses = [(h['step'], h['eval_loss']) for h in history if 'eval_loss' in h]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nðŸ“Š Dataset: {len(tokenized_train)} train, {len(tokenized_eval)} eval\")\n",
        "print(f\"\\nðŸ“‰ Training Loss: {train_losses[0][1]:.4f} â†’ {train_losses[-1][1]:.4f}\")\n",
        "print(f\"ðŸ“ˆ Validation Loss: {eval_losses[0][1]:.4f} â†’ {min(e[1] for e in eval_losses):.4f} (best)\")\n",
        "print(f\"ðŸŽ¯ Perplexity: {perplexity:.2f}\")\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    \"perplexity\": perplexity,\n",
        "    \"train_samples\": len(tokenized_train),\n",
        "    \"eval_samples\": len(tokenized_eval),\n",
        "    \"final_train_loss\": train_losses[-1][1] if train_losses else None,\n",
        "    \"best_eval_loss\": min(e[1] for e in eval_losses) if eval_losses else None,\n",
        "    \"test_questions\": TEST_QUESTIONS,\n",
        "    \"base_responses\": all_base_responses,\n",
        "    \"finetuned_responses\": all_ft_responses\n",
        "}\n",
        "\n",
        "import os\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "with open(f\"{OUTPUT_DIR}/evaluation_results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(f\"\\nSaved to {OUTPUT_DIR}/evaluation_results.json\")"
      ],
      "metadata": {
        "id": "summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14. Create Model Card & Push Metrics to HuggingFace"
      ],
      "metadata": {
        "id": "push_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if PUSH_TO_HUB:\n",
        "    from huggingface_hub import HfApi\n",
        "    \n",
        "    # Log metrics\n",
        "    metrics = {\n",
        "        \"eval_loss\": results[\"best_eval_loss\"],\n",
        "        \"perplexity\": round(perplexity, 2),\n",
        "        \"train_samples\": results[\"train_samples\"],\n",
        "        \"eval_samples\": results[\"eval_samples\"]\n",
        "    }\n",
        "    trainer.log_metrics(\"eval\", metrics)\n",
        "    trainer.save_metrics(\"eval\", metrics)\n",
        "    \n",
        "    # Create model card\n",
        "    model_card = f\"\"\"---\n",
        "language:\n",
        "- en\n",
        "license: apache-2.0\n",
        "tags:\n",
        "- agriculture\n",
        "- phi-2\n",
        "- qlora\n",
        "- instruction-tuning\n",
        "datasets:\n",
        "- AI4Agr/CROP-dataset\n",
        "base_model: microsoft/phi-2\n",
        "model-index:\n",
        "- name: AgricGPT-Phi2\n",
        "  results:\n",
        "  - task:\n",
        "      type: text-generation\n",
        "      name: Agricultural Q&A\n",
        "    metrics:\n",
        "    - type: perplexity\n",
        "      value: {perplexity:.2f}\n",
        "    - type: loss\n",
        "      value: {results['best_eval_loss']:.4f}\n",
        "---\n",
        "\n",
        "# AgricGPT - Agricultural Domain Language Model\n",
        "\n",
        "Fine-tuned **Microsoft Phi-2** for agricultural Q&A using QLoRA.\n",
        "\n",
        "## Evaluation Results\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| **Perplexity** | {perplexity:.2f} |\n",
        "| **Validation Loss** | {results['best_eval_loss']:.4f} |\n",
        "| **Training Samples** | {results['train_samples']:,} |\n",
        "| **Validation Samples** | {results['eval_samples']:,} |\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
        "model = PeftModel.from_pretrained(base, \"{HF_MODEL_NAME}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
        "\n",
        "prompt = \"### Instruction:\\\\nWhat is crop rotation?\\\\n\\\\n### Response:\\\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=150)\n",
        "print(tokenizer.decode(outputs[0]))\n",
        "```\n",
        "\n",
        "## Example\n",
        "\n",
        "**Q: What is crop rotation?**\n",
        "> {all_ft_responses[0][:300]}...\n",
        "\"\"\"\n",
        "    \n",
        "    with open(f\"{OUTPUT_DIR}/README.md\", \"w\") as f:\n",
        "        f.write(model_card)\n",
        "    \n",
        "    # Push everything\n",
        "    print(\"Pushing model with metrics to HuggingFace...\")\n",
        "    trainer.push_to_hub()\n",
        "    \n",
        "    api = HfApi()\n",
        "    username = api.whoami()['name']\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=f\"{OUTPUT_DIR}/README.md\",\n",
        "        path_in_repo=\"README.md\",\n",
        "        repo_id=f\"{username}/{HF_MODEL_NAME}\",\n",
        "        repo_type=\"model\"\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nâœ… Done! View at: https://huggingface.co/{username}/{HF_MODEL_NAME}\")\n",
        "    print(\"Metrics are now visible on your model card!\")"
      ],
      "metadata": {
        "id": "push_metrics"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
