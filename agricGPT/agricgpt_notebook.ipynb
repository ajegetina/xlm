{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AgricGPT - Agricultural Domain Instruction Tuning with QLoRA\n",
        "\n",
        "This notebook fine-tunes **Microsoft Phi-2** on the **AI4Agr/CROP-dataset** for agricultural Q&A using:\n",
        "- **QLoRA** (4-bit quantization + Low-Rank Adaptation)\n",
        "- **Instruction tuning** format\n",
        "\n",
        "**Requirements**: T4 GPU or better"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "Run this cell and **restart the runtime** if prompted."
      ],
      "metadata": {
        "id": "install_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "    torch>=2.0.0 \\\n",
        "    transformers>=4.40.0 \\\n",
        "    datasets>=2.0.0 \\\n",
        "    peft>=0.10.0 \\\n",
        "    bitsandbytes>=0.43.0 \\\n",
        "    accelerate>=0.27.0 \\\n",
        "    huggingface_hub"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Configuration\n",
        "\n",
        "All hyperparameters in one place for easy experimentation."
      ],
      "metadata": {
        "id": "config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Model\n",
        "MODEL_NAME = \"microsoft/phi-2\"\n",
        "OUTPUT_DIR = \"./agri_model_results\"\n",
        "\n",
        "# Hugging Face Hub\n",
        "HF_MODEL_NAME = \"agricgpt-phi2\"  # <- Change this to your desired name!\n",
        "\n",
        "# Dataset\n",
        "DATASET_SIZE = 5000  # Set to None for full dataset\n",
        "MAX_SEQ_LENGTH = 512\n",
        "\n",
        "# LoRA\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "TARGET_MODULES = [\"fc1\", \"fc2\", \"q_proj\", \"k_proj\", \"v_proj\", \"dense\"]\n",
        "\n",
        "# Training\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 2\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "LOGGING_STEPS = 10\n",
        "\n",
        "# Check GPU\n",
        "if not torch.cuda.is_available():\n",
        "    raise ValueError(\"GPU required! Enable T4 GPU in Runtime > Change runtime type\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load Model with 4-bit Quantization"
      ],
      "metadata": {
        "id": "model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Set seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# 4-bit quantization config (QLoRA)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        "    device_map={\"\":0}\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "print(f\"Model loaded: {MODEL_NAME}\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Load and Prepare Dataset"
      ],
      "metadata": {
        "id": "dataset_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load AI4Agr/CROP-dataset (English subset)\n",
        "dataset = load_dataset(\n",
        "    \"AI4Agr/CROP-dataset\",\n",
        "    data_files=\"**/*_en/**/*.json\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "# Limit for pilot run\n",
        "if DATASET_SIZE:\n",
        "    dataset = dataset.select(range(min(DATASET_SIZE, len(dataset))))\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)} samples\")\n",
        "print(f\"Sample: {dataset[0]}\")"
      ],
      "metadata": {
        "id": "load_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format as instruction-response pairs with EOS token\n",
        "def format_instruction(sample):\n",
        "    \"\"\"Format sample as instruction-response with EOS token for clean stopping.\"\"\"\n",
        "    prompt = (\n",
        "        f\"### Instruction:\\n{sample['instruction']}\\n\\n\"\n",
        "        f\"### Response:\\n{sample['output']}{tokenizer.eos_token}\"\n",
        "    )\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "dataset = dataset.map(format_instruction)\n",
        "\n",
        "# Show formatted example\n",
        "print(\"Formatted example:\")\n",
        "print(dataset[0][\"text\"][:500])"
      ],
      "metadata": {
        "id": "format_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LENGTH,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "\n",
        "print(f\"Tokenized dataset ready: {len(tokenized_dataset)} samples\")"
      ],
      "metadata": {
        "id": "tokenize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Configure LoRA Adapters"
      ],
      "metadata": {
        "id": "lora_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")"
      ],
      "metadata": {
        "id": "lora_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training"
      ],
      "metadata": {
        "id": "training_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Inference Helper"
      ],
      "metadata": {
        "id": "inference_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GenerationConfig, pipeline, logging\n",
        "\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "model.eval()\n",
        "\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=256,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "def ask_agrigpt(question: str) -> str:\n",
        "    \"\"\"Ask AgricGPT a question about agriculture.\"\"\"\n",
        "    prompt = f\"### Instruction:\\n{question}\\n\\n### Response:\\n\"\n",
        "    result = pipe(prompt, generation_config=generation_config)\n",
        "    response = result[0]['generated_text'].split(\"### Response:\\n\")[-1]\n",
        "    response = response.split(\"### Instruction:\")[0].strip()\n",
        "    return response\n",
        "\n",
        "print(\"Inference helper ready!\")"
      ],
      "metadata": {
        "id": "inference_setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Test the Model"
      ],
      "metadata": {
        "id": "test_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with crop rotation question\n",
        "torch.manual_seed(42)\n",
        "question = \"What is crop rotation?\"\n",
        "print(f\"Q: {question}\")\n",
        "print(f\"A: {ask_agrigpt(question)}\")"
      ],
      "metadata": {
        "id": "test1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with soil erosion question\n",
        "torch.manual_seed(42)\n",
        "question = \"How can I prevent soil erosion on my farm?\"\n",
        "print(f\"Q: {question}\")\n",
        "print(f\"A: {ask_agrigpt(question)}\")"
      ],
      "metadata": {
        "id": "test2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with organic farming question\n",
        "torch.manual_seed(42)\n",
        "question = \"What are the benefits of organic farming?\"\n",
        "print(f\"Q: {question}\")\n",
        "print(f\"A: {ask_agrigpt(question)}\")"
      ],
      "metadata": {
        "id": "test3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Save Model Locally"
      ],
      "metadata": {
        "id": "save_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model locally\n",
        "save_path = f\"{OUTPUT_DIR}/final_model\"\n",
        "trainer.save_model(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "print(f\"Model saved to: {save_path}\")"
      ],
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Push to Hugging Face Hub ðŸ¤—\n",
        "\n",
        "Upload your trained model to your Hugging Face account.\n",
        "\n",
        "1. Get your token at: https://huggingface.co/settings/tokens\n",
        "2. Make sure you have **write** access enabled"
      ],
      "metadata": {
        "id": "hf_hub_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Login to Hugging Face (will prompt for your token)\n",
        "login()"
      ],
      "metadata": {
        "id": "hf_login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push model and tokenizer to Hugging Face Hub\n",
        "print(f\"Pushing model to Hugging Face as '{HF_MODEL_NAME}'...\")\n",
        "\n",
        "model.push_to_hub(HF_MODEL_NAME)\n",
        "tokenizer.push_to_hub(HF_MODEL_NAME)\n",
        "\n",
        "print(f\"\\nâœ… Model uploaded successfully!\")\n",
        "print(f\"View at: https://huggingface.co/YOUR_USERNAME/{HF_MODEL_NAME}\")"
      ],
      "metadata": {
        "id": "push_to_hub"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
