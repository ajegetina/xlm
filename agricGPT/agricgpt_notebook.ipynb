{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AgricGPT - Agricultural Domain Instruction Tuning with QLoRA\n",
        "\n",
        "Fine-tunes **Microsoft Phi-2** on **AI4Agr/CROP-dataset** with automatic checkpoint pushing to Hugging Face Hub.\n",
        "\n",
        "**Features**:\n",
        "- QLoRA (4-bit quantization)\n",
        "- Checkpoints pushed to HF every N steps (fault-tolerant)\n",
        "- Before/after training comparison"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Dependencies"
      ],
      "metadata": {
        "id": "install_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch transformers datasets peft bitsandbytes accelerate huggingface_hub"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Login to Hugging Face (FIRST!)\n",
        "\n",
        "Login early so checkpoints can be pushed during training."
      ],
      "metadata": {
        "id": "hf_login_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Get your token at: https://huggingface.co/settings/tokens\n",
        "# Make sure it has WRITE access!\n",
        "login()"
      ],
      "metadata": {
        "id": "hf_login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Configuration"
      ],
      "metadata": {
        "id": "config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Model\n",
        "MODEL_NAME = \"microsoft/phi-2\"\n",
        "OUTPUT_DIR = \"./agri_model_results\"\n",
        "\n",
        "# Hugging Face Hub - CHANGE THIS!\n",
        "HF_MODEL_NAME = \"agricgpt-phi2\"  # Your model name on HF\n",
        "PUSH_TO_HUB = True\n",
        "SAVE_STEPS = 100  # Push checkpoint every N steps\n",
        "\n",
        "# Dataset\n",
        "DATASET_SIZE = 5000\n",
        "MAX_SEQ_LENGTH = 512\n",
        "\n",
        "# LoRA\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "TARGET_MODULES = [\"fc1\", \"fc2\", \"q_proj\", \"k_proj\", \"v_proj\", \"dense\"]\n",
        "\n",
        "# Training\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 2\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "LOGGING_STEPS = 10\n",
        "\n",
        "# GPU check\n",
        "assert torch.cuda.is_available(), \"GPU required!\"\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Load Model"
      ],
      "metadata": {
        "id": "model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        "    device_map={\"\":0}\n",
        ")\n",
        "model.config.use_cache = False\n",
        "print(f\"Loaded: {MODEL_NAME}\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Base Model Output (BEFORE Training)"
      ],
      "metadata": {
        "id": "base_model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GenerationConfig, pipeline\n",
        "\n",
        "base_pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "base_gen_config = GenerationConfig(\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "test_prompt = \"### Instruction:\\nWhat is crop rotation?\\n\\n### Response:\\n\"\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"BASE MODEL (before training)\")\n",
        "print(\"=\" * 50)\n",
        "torch.manual_seed(42)\n",
        "result = base_pipe(test_prompt, generation_config=base_gen_config)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "base_model_test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Prepare Dataset"
      ],
      "metadata": {
        "id": "dataset_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"AI4Agr/CROP-dataset\", data_files=\"**/*_en/**/*.json\", split=\"train\")\n",
        "if DATASET_SIZE:\n",
        "    dataset = dataset.select(range(min(DATASET_SIZE, len(dataset))))\n",
        "\n",
        "def format_instruction(sample):\n",
        "    return {\"text\": f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']}{tokenizer.eos_token}\"}\n",
        "\n",
        "dataset = dataset.map(format_instruction)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_SEQ_LENGTH, padding=\"max_length\")\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
        "print(f\"Dataset: {len(tokenized_dataset)} samples\")"
      ],
      "metadata": {
        "id": "prepare_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Configure LoRA"
      ],
      "metadata": {
        "id": "lora_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "peft_config = LoraConfig(\n",
        "    r=LORA_R, lora_alpha=LORA_ALPHA, target_modules=TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")"
      ],
      "metadata": {
        "id": "lora_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Training (with automatic HF checkpoint pushing)\n",
        "\n",
        "Checkpoints are pushed to Hugging Face every `SAVE_STEPS` steps.\n",
        "\n",
        "If training is interrupted, you can resume from the last checkpoint!"
      ],
      "metadata": {
        "id": "training_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    # Checkpoint saving & HF pushing\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=3,  # Keep only last 3 checkpoints locally\n",
        "    push_to_hub=PUSH_TO_HUB,\n",
        "    hub_model_id=HF_MODEL_NAME if PUSH_TO_HUB else None,\n",
        "    hub_strategy=\"every_save\",  # Push at every checkpoint!\n",
        "    report_to=\"none\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "print(f\"Training with checkpoints pushed to HF every {SAVE_STEPS} steps...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Fine-Tuned Model Output (AFTER Training)"
      ],
      "metadata": {
        "id": "post_training_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import logging\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "model.eval()\n",
        "\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "gen_config = GenerationConfig(\n",
        "    max_new_tokens=256, do_sample=True, temperature=0.7,\n",
        "    top_p=0.9, repetition_penalty=1.2,\n",
        "    eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "def ask_agrigpt(q):\n",
        "    prompt = f\"### Instruction:\\n{q}\\n\\n### Response:\\n\"\n",
        "    result = pipe(prompt, generation_config=gen_config)\n",
        "    return result[0]['generated_text'].split(\"### Response:\\n\")[-1].split(\"### Instruction:\")[0].strip()\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"FINE-TUNED MODEL (after training)\")\n",
        "print(\"=\" * 50)\n",
        "torch.manual_seed(42)\n",
        "print(f\"Q: What is crop rotation?\")\n",
        "print(f\"A: {ask_agrigpt('What is crop rotation?')}\")"
      ],
      "metadata": {
        "id": "post_training_test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Push Final Model"
      ],
      "metadata": {
        "id": "final_push_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Push final model to HF Hub\n",
        "if PUSH_TO_HUB:\n",
        "    print(f\"Pushing final model to {HF_MODEL_NAME}...\")\n",
        "    trainer.push_to_hub()\n",
        "    print(f\"âœ… Done! View at: https://huggingface.co/YOUR_USERNAME/{HF_MODEL_NAME}\")"
      ],
      "metadata": {
        "id": "final_push"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Resume Training from Checkpoint (if interrupted)\n",
        "\n",
        "If training was interrupted, run this cell to resume from the last checkpoint."
      ],
      "metadata": {
        "id": "resume_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment and run to resume from last checkpoint:\n",
        "# trainer.train(resume_from_checkpoint=True)"
      ],
      "metadata": {
        "id": "resume_training"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
